mat <- matrix(rnorm(100), nrow = 10 )
heatmap(mat)
library(shiny)
install.packages(shiny)
install.packages("shiny")
library(ggplot2)
library(sf)
library(osmdata)
bbox <- c(80.041, 12.820, 80.050, 12.825)
roads <- opq(bbox) %>% add_osm_feature("highway") %>% osmdata_sf()
buildings <- opq(bbox) %>% add_osm_feature("building") %>% osmdata_sf()
water <- opq(bbox) %>% add_osm_feature("natural", value = "water") %>% osmdata_sf()
wifi_datareal <- read.csv("wifi_data.csv")
# Load libraries (if not already loaded)
library(osmdata)
library(sf)
library(ggplot2)
# Define bounding box around your data
bbox <- c(80.041, 12.820, 80.050, 12.825)
# Download base map layers
roads <- opq(bbox) %>% add_osm_feature("highway") %>% osmdata_sf()
paths <- opq(bbox) %>% add_osm_feature("highway", value = c("footway", "path", "cycleway")) %>% osmdata_sf()
buildings <- opq(bbox) %>% add_osm_feature("building") %>% osmdata_sf()
water <- opq(bbox) %>% add_osm_feature("natural", value = "water") %>% osmdata_sf()
# Your actual Wi-Fi data
wifi_datareal <- data.frame(
lat = c(12.823, 12.821, 12.824 , 12.822),
lon = c(80.042, 80.043, 80.046 , 80.049),
count = c(15, 8, 5, 12)
)
# ðŸ”¥ Heatmap Plot
ggplot() +
# Map layers
geom_sf(data = water$osm_polygons, fill = "lightblue", color = NA) +
geom_sf(data = buildings$osm_polygons, fill = "gray80", color = NA) +
geom_sf(data = roads$osm_lines, color = "gray40", size = 0.4) +
geom_sf(data = paths$osm_lines, color = "blue", size = 0.3, alpha = 0.6) +
# ðŸ”¥ Heatmap layer (density weighted by Wi-Fi count)
stat_density_2d(data = wifi_datareal,
aes(x = lon, y = lat, fill = ..level.., alpha = ..level.., weight = count),
geom = "polygon", contour = TRUE) +
# Optional: show actual points
geom_point(data = wifi_datareal, aes(x = lon, y = lat), color = "red", size = 2) +
scale_fill_viridis_c(option = "inferno") +
scale_alpha(range = c(0.3, 0.8)) +
coord_sf(xlim = c(80.041, 80.050), ylim = c(12.820, 12.825), expand = FALSE) +
theme_minimal() +
labs(title = "Wi-Fi User Density Heatmap on Campus", fill = "Density", alpha = "Density")
library(osmdata)
library(sf)
library(ggplot2)
bbox <- c(80.041, 12.820, 80.050, 12.825)
roads <- opq(bbox) %>% add_osm_feature("highway") %>% osmdata_sf()
paths <- opq(bbox) %>% add_osm_feature("highway", value = c("footway", "path", "cycleway")) %>% osmdata_sf()
buildings <- opq(bbox) %>% add_osm_feature("building") %>% osmdata_sf()
water <- opq(bbox) %>% add_osm_feature("natural", value = "water") %>% osmdata_sf()
wifi_datareal <- data.frame(
lat = c(12.823, 12.821, 12.824 , 12.822),
lon = c(80.042, 80.043, 80.046 , 80.049),
count = c(15, 8, 5, 12)
)
ggplot() +
# Map layers
geom_sf(data = water$osm_polygons, fill = "lightblue", color = NA) +
geom_sf(data = buildings$osm_polygons, fill = "gray80", color = NA) +
geom_sf(data = roads$osm_lines, color = "gray40", size = 0.4) +
geom_sf(data = paths$osm_lines, color = "blue", size = 0.3, alpha = 0.6) +
stat_density_2d(data = wifi_datareal,
aes(x = lon, y = lat, fill = ..level.., alpha = ..level.., weight = count),
geom = "polygon", contour = TRUE) +
geom_point(data = wifi_datareal, aes(x = lon, y = lat), color = "red", size = 2) +
scale_fill_viridis_c(option = "inferno") +
scale_alpha(range = c(0.3, 0.8)) +
coord_sf(xlim = c(80.041, 80.050), ylim = c(12.820, 12.825), expand = FALSE) +
theme_minimal() +
labs(title = "Wi-Fi User Density Heatmap on Campus", fill = "Density", alpha = "Density")
install(ggplot)
library(tidyverse)
install.packages("tidyverse")
library(tidyverse)
data <- read_csv("C:/Users/arjun/Downloads/comments.csv", header = FALSE, stringsAsFactors = FALSE)
data <- read_csv("comments.csv", col_names = FALSE)
data <- read_csv("C:/Users/arjun/Downloads/comments.csv", col_names = FALSE)
View(C:/Users/arjun/Downloads/comments.csv)
View("C:/Users/arjun/Downloads/comments.csv")
View(data)
colnames(data) <- c ("id","author","comment")
colnames(data)
data <- read.csv("C:/Users/arjun/Downloads/comments.csv", header = FALSE)
data <- read.csv("C:/Users/arjun/Downloads/comments.csv", header = FALSE)
data <- read.csv("C:/Users/arjun/Downloads/comments.csv", header = FALSE)
colnames(data) <- c ("id","author","date","content")
library(textclean)
data <- data %>%
mutate(clean_comment = content %>%
tolower() %>%                                # lowercase
replace_contraction() %>%                    # expand contractions (don't â†’ do not)
replace_symbol() %>%                         # replace symbols with words
replace_number(remove = TRUE) %>%            # remove numbers
replace_non_ascii() %>%                      # fix/remove non-ASCII chars
replace_url() %>%                            # remove URLs
replace_emoji() %>%                          # convert emojis to text (ðŸ˜Š â†’ smile)
strip()                                      # trim spaces
)
na.omit()
cdata <- na.omit(data)
data <- cdata %>%
mutate(clean_comment = content %>%
tolower() %>%                                # lowercase
replace_contraction() %>%                    # expand contractions (don't â†’ do not)
replace_symbol() %>%                         # replace symbols with words
replace_number(remove = TRUE) %>%            # remove numbers
replace_non_ascii() %>%                      # fix/remove non-ASCII chars
replace_url() %>%                            # remove URLs
replace_emoji() %>%                          # convert emojis to text (ðŸ˜Š â†’ smile)
strip()                                      # trim spaces
)
library(textclean)
# Overwrite comment column
data$clean_comment <- data$content %>%
tolower() %>%                  # lowercase
replace_contraction() %>%      # expand contractions
replace_symbol() %>%           # replace symbols
replace_number(remove = TRUE) %>%
replace_non_ascii() %>%        # fix/remove non-ASCII chars
replace_url() %>%              # remove URLs
replace_emoji() %>%            # convert emojis to words
strip()                        # trim space
library(text2vec)
tokens <- itoken(data$clean_comment,
tokenizer = word_tokenizer,
progressbar = TRUE)
vocab <- create_vocabulary(tokens)
vocab <- prune_vocabulary(vocab, term_count_min = 5) # keep words seen â‰¥5 times
vectorizer <- vocab_vectorizer(vocab)
dtm <- create_dtm(tokens, vectorizer)
View(dtm)
bing <- get_sentiments("bing")
install.packages("tidytext")
bing <- get_sentiments("bing")
library(tidytext)
bing <- get_sentiments("bing")
View(bing)
labeled_words <- tokens %>%
inner_join(bing, by = "word")
tokens <- data %>%
unnest_tokens(word, clean_comment)
labeled_words <- tokens %>%
inner_join(bing, by = "word")
View(tokens)
View(labeled_words)
View(labeled_words)
comment_sentiment <- labeled_words %>%
count(comment_id, sentiment) %>%
pivot_wider(names_from = sentiment, values_from = n, values_fill = 0)
comment_sentiment <- labeled_words %>%
count(id, sentiment) %>%
pivot_wider(names_from = sentiment, values_from = n, values_fill = 0)
colnames(data)[5] <- "o"
comment_sentiment <- labeled_words %>%
count(id, sentiment) %>%
pivot_wider(names_from = sentiment, values_from = n, values_fill = 0)
colnames(data)[5] <- "new_name"   # give a proper name to the 5th column
data <- as_tibble(data)
comment_sentiment <- comment_sentiment %>%
mutate(label = ifelse(positive > negative, "positive",
ifelse(negative > positive, "negative", "neutral")))
comment_sentiment <- labeled_words %>%
count(comment_id, sentiment) %>%
pivot_wider(names_from = sentiment, values_from = n, values_fill = 0)
comment_sentiment <- labeled_words %>%
count(id, sentiment) %>%
pivot_wider(names_from = sentiment, values_from = n, values_fill = 0)
colnames(labeled_words)[5] <- "o"
comment_sentiment <- labeled_words %>%
count(id, sentiment) %>%
pivot_wider(names_from = sentiment, values_from = n, values_fill = 0)
View(comment_sentiment)
comment_sentiment <- comment_sentiment %>%
mutate(label = ifelse(positive > negative, "positive",
ifelse(negative > positive, "negative", "neutral")))
save.image("C:/Users/arjun/Downloads/commproj.RData")
install.packages(e1071)
install.packages("e1071")
trainIndex <- createDataPartition(data$label, p = 0.8, list = FALSE)
install.packages("caret")
library(caret)
trainIndex <- createDataPartition(data$label, p = 0.8, list = FALSE)
table(data$label)
data$label <- sample(c("positive", "negative"), nrow(data), replace = TRUE)
trainIndex <- createDataPartition(data$label, p = 0.8, list = FALSE)
train_dtm <- dtm_tfidf[trainIndex, ]
library(tidytext)
library(dplyr)
# Assume your cleaned data is in `data` with a column `comment`
tokens <- data %>%
unnest_tokens(word, comment) %>%
anti_join(stop_words, by = "word")   # remove stopwords
library(caret)
set.seed(123)
trainIndex <- createDataPartition(data$label, p = 0.8, list = FALSE)
train_dtm <- dtm_tfidf[trainIndex, ]
library(caret)
set.seed(123)
trainIndex <- createDataPartition(data$label, p = 0.8, list = FALSE)
train_dtm <- dtm[trainIndex, ]
test_dtm  <- dtm[-trainIndex, ]
train_labels <- data$label[trainIndex]
test_labels  <- data$label[-trainIndex]
library(e1071)
# NB model
model_nb <- naiveBayes(as.matrix(train_dtm), as.factor(train_labels))
pred <- predict(model_nb, as.matrix(test_dtm))
confusionMatrix(pred, as.factor(test_labels))
library(text2vec)
library(caret)
library(e1071)
# Tokenize with unigrams + bigrams
tokens <- itoken(data$clean_comment,
tokenizer = word_tokenizer,
progressbar = TRUE)
vectorizer <- vocab_vectorizer(
create_vocabulary(tokens, ngram = c(1L, 2L))
)
dtm <- create_dtm(tokens, vectorizer)
# Apply TF-IDF
tfidf <- TfIdf$new()
dtm_tfidf <- tfidf$fit_transform(dtm)
trainIndex <- createDataPartition(data$label, p = 0.8, list = FALSE)
dtm_train <- dtm_tfidf[trainIndex, ]
dtm_test  <- dtm_tfidf[-trainIndex, ]
train_labels <- data$label[trainIndex]
test_labels  <- data$label[-trainIndex]
svm_model <- svm(x = dtm_train, y = train_labels, kernel = "linear", probability = TRUE)
svm_model <- svm(x = dtm_train, y = train_labels, kernel = "linear", probability = TRUE)
install.packages("SparseM")
library(SparseM)
svm_model <- svm(x = dtm_train, y = train_labels, kernel = "linear", probability = TRUE)
# Make sure labels are factors
train_labels <- as.factor(train_labels)
test_labels  <- as.factor(test_labels)
# Train SVM for classification
svm_model <- svm(x = dtm_train, y = train_labels,
kernel = "linear", probability = TRUE)
# Predict
pred <- predict(svm_model, dtm_test)
# Evaluate
confusionMatrix(pred, test_labels)
library(glmnet)
library(glmnet)
install.packages("glmnet")
library(glmnet)
log_reg <- cv.glmnet(x = dtm_train, y = train_labels,
family = "binomial", type.measure = "class")
pred_lr <- predict(log_reg, dtm_test, s = "lambda.min", type = "class")
confusionMatrix(as.factor(pred_lr), test_labels)
library(tm)
install.packages("tm")
library(tm)
# Assuming 'dtm' is your count-based DocumentTermMatrix
dtm_tfidf <- weightTfIdf(dtm)
library(tm)
# Assuming 'dtm' is your count-based DocumentTermMatrix
dtm_tfidf <- weightTfIdf(dtm)
install.packages("nDocs")
library(tm)
# Assuming 'dtm' is your count-based DocumentTermMatrix
dtm_tfidf <- weightTfIdf(dtm)
library(nDocs)
library(text2vec)
# assume 'dtm' is a dgCMatrix from text2vec
tfidf_transformer <- TfIdf$new()
dtm_tfidf <- tfidf_transformer$fit_transform(dtm)
library(text2vec)
library(caret)
library(e1071)
# 1. Tokenization + DTM creation
tokens <- word_tokenizer(data$clean_comment)
it <- itoken(tokens, progressbar = TRUE)
vocab <- create_vocabulary(it)
vectorizer <- vocab_vectorizer(vocab)
dtm <- create_dtm(it, vectorizer)   # sparse matrix (dgCMatrix)
# 2. Apply TF-IDF
tfidf <- TfIdf$new()
dtm_tfidf <- tfidf$fit_transform(dtm)
# 3. Train-test split
set.seed(123)
trainIndex <- createDataPartition(data$label, p = 0.8, list = FALSE)
dtm_train <- dtm_tfidf[trainIndex, ]
dtm_test  <- dtm_tfidf[-trainIndex, ]
train_labels <- as.factor(data$label[trainIndex])
test_labels  <- as.factor(data$label[-trainIndex])
# 4. Train SVM model
svm_model <- svm(x = dtm_train, y = train_labels,
kernel = "linear", probability = TRUE)
# 5. Predictions
pred <- predict(svm_model, dtm_test)
# 6. Confusion Matrix
confusionMatrix(pred, test_labels)
library(text2vec)
library(tm)
library(caret)
library(glmnet)
set.seed(123)
# --- Step 1: Tokenization with stopword removal ---
prep_fun <- tolower
tok_fun <- word_tokenizer
it <- itoken(
data$clean_comment,
preprocessor = prep_fun,
tokenizer = function(x) tok_fun(removeWords(x, stopwords("en"))),
progressbar = TRUE
)
# --- Step 2: Vocabulary (unigrams + bigrams) ---
vocab <- create_vocabulary(it, ngram = c(1L, 2L))
vectorizer <- vocab_vectorizer(vocab)
# --- Step 3: Document-Term Matrix with TF-IDF ---
dtm <- create_dtm(it, vectorizer)
tfidf <- TfIdf$new()
dtm_tfidf <- tfidf$fit_transform(dtm)
# --- Step 4: Train/Test Split ---
trainIndex <- createDataPartition(data$label, p = 0.8, list = FALSE)
dtm_train <- dtm_tfidf[trainIndex, ]
dtm_test  <- dtm_tfidf[-trainIndex, ]
train_labels <- data$label[trainIndex]
test_labels  <- data$label[-trainIndex]
# --- Step 5: Logistic Regression Training ---
logreg <- cv.glmnet(x = dtm_train, y = train_labels,
family = "binomial", alpha = 0)
# --- Step 6: Prediction + Confusion Matrix ---
pred <- predict(logreg, dtm_test, type = "class")
confusionMatrix(as.factor(pred), as.factor(test_labels))
load("C:/Users/arjun/OneDrive/Documents/GitHub/Commproj/commproj.RData")
